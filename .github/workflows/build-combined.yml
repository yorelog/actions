name: Build PyTorch 2.8.0 + vLLM 0.10.0 for CUDA 12.1

on:
  workflow_dispatch:
    inputs:
      pytorch_version:
        description: 'PyTorch版本'
        required: true
        default: '2.8.0'
        type: string
      vllm_version:
        description: 'vLLM版本'
        required: true
        default: '0.10.0'
        type: string
      gpu_arch:
        description: 'CUDA架构'
        required: true
        default: '8.0;8.6;8.9'
        type: string
      build_pytorch:
        description: '构建PyTorch'
        required: true
        default: true
        type: boolean
      build_vllm:
        description: '构建vLLM'
        required: true
        default: true
        type: boolean
  push:
    tags:
      - 'release-*'

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}/cu121-builder

jobs:
  prepare:
    runs-on: ubuntu-latest
    outputs:
      pytorch_version: ${{ steps.versions.outputs.pytorch_version }}
      vllm_version: ${{ steps.versions.outputs.vllm_version }}
      gpu_arch: ${{ steps.versions.outputs.gpu_arch }}
      build_pytorch: ${{ steps.versions.outputs.build_pytorch }}
      build_vllm: ${{ steps.versions.outputs.build_vllm }}
    
    steps:
    - name: Set version outputs
      id: versions
      run: |
        echo "pytorch_version=${{ inputs.pytorch_version || '2.8.0' }}" >> $GITHUB_OUTPUT
        echo "vllm_version=${{ inputs.vllm_version || '0.10.0' }}" >> $GITHUB_OUTPUT
        echo "gpu_arch=${{ inputs.gpu_arch || '8.0;8.6;8.9' }}" >> $GITHUB_OUTPUT
        echo "build_pytorch=${{ inputs.build_pytorch || 'true' }}" >> $GITHUB_OUTPUT
        echo "build_vllm=${{ inputs.build_vllm || 'true' }}" >> $GITHUB_OUTPUT

  build-environment:
    runs-on: self-hosted
    needs: prepare
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Build Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        push: false
        tags: cu121-builder:latest
        cache-from: type=gha
        cache-to: type=gha,mode=max
        platforms: linux/amd64
    
    - name: Test environment
      run: |
        echo "Testing build environment..."
        docker run --gpus all --rm cu121-builder:latest /scripts/verify_env.sh || echo "GPU not available in CI"
        
        echo "Listing available versions..."
        docker run --rm cu121-builder:latest /scripts/list_versions.sh all

  build-pytorch:
    if: needs.prepare.outputs.build_pytorch == 'true'
    runs-on: self-hosted
    needs: [prepare, build-environment]
    
    strategy:
      matrix:
        include:
          - gpu_type: "A100"
            cuda_arch: "8.0"
          - gpu_type: "RTX3090Ti"
            cuda_arch: "8.6"
          - gpu_type: "H20"
            cuda_arch: "8.9"
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Create output directory
      run: mkdir -p output
    
    - name: Build PyTorch ${{ needs.prepare.outputs.pytorch_version }}
      run: |
        echo "Building PyTorch ${{ needs.prepare.outputs.pytorch_version }} for ${{ matrix.gpu_type }}"
        
        docker run --gpus all --rm \
          -v $(pwd)/output:/output \
          -v $(pwd)/workspace:/workspace \
          -e TORCH_CUDA_ARCH_LIST="${{ matrix.cuda_arch }}" \
          -e CUDA_ARCH_LIST="${{ matrix.cuda_arch }}" \
          cu121-builder:latest \
          /scripts/build_optimized.sh pytorch ${{ needs.prepare.outputs.pytorch_version }}
      timeout-minutes: 480
    
    - name: Test PyTorch build
      run: |
        if ls output/torch-*.whl 1> /dev/null 2>&1; then
          echo "✅ PyTorch wheel构建成功"
          docker run --gpus all --rm -v $(pwd)/output:/wheels cu121-builder:latest bash -c "
            pip install /wheels/torch-*.whl && \
            python3 -c 'import torch; print(f\"PyTorch {torch.__version__} installed successfully\")'
          "
        else
          echo "❌ PyTorch wheel构建失败"
          exit 1
        fi
    
    - name: Upload PyTorch artifacts
      uses: actions/upload-artifact@v4
      with:
        name: pytorch-${{ needs.prepare.outputs.pytorch_version }}-${{ matrix.gpu_type }}
        path: output/torch-*.whl
        retention-days: 30

  build-vllm:
    if: needs.prepare.outputs.build_vllm == 'true'
    runs-on: self-hosted
    needs: [prepare, build-environment, build-pytorch]
    # 如果不构建PyTorch，则不依赖build-pytorch
    # needs: [prepare, build-environment]
    
    strategy:
      matrix:
        include:
          - gpu_type: "A100"
            cuda_arch: "8.0"
          - gpu_type: "RTX3090Ti"
            cuda_arch: "8.6"
          - gpu_type: "H20"
            cuda_arch: "8.9"
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Create output directory
      run: mkdir -p output
    
    - name: Download PyTorch wheel (if built)
      if: needs.prepare.outputs.build_pytorch == 'true'
      uses: actions/download-artifact@v4
      with:
        name: pytorch-${{ needs.prepare.outputs.pytorch_version }}-${{ matrix.gpu_type }}
        path: pytorch-wheels/
    
    - name: Build vLLM ${{ needs.prepare.outputs.vllm_version }}
      run: |
        echo "Building vLLM ${{ needs.prepare.outputs.vllm_version }} for ${{ matrix.gpu_type }}"
        
        # 准备PyTorch安装命令
        if [ "${{ needs.prepare.outputs.build_pytorch }}" == "true" ] && [ -f pytorch-wheels/torch-*.whl ]; then
          PYTORCH_INSTALL="pip install /pytorch-wheels/torch-*.whl"
          PYTORCH_MOUNT="-v $(pwd)/pytorch-wheels:/pytorch-wheels"
        else
          PYTORCH_INSTALL="pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
          PYTORCH_MOUNT=""
        fi
        
        docker run --gpus all --rm \
          -v $(pwd)/output:/output \
          -v $(pwd)/workspace:/workspace \
          $PYTORCH_MOUNT \
          -e TORCH_CUDA_ARCH_LIST="${{ matrix.cuda_arch }}" \
          -e CUDA_ARCH_LIST="${{ matrix.cuda_arch }}" \
          cu121-builder:latest bash -c "
            $PYTORCH_INSTALL
            /scripts/build_optimized.sh vllm ${{ needs.prepare.outputs.vllm_version }}
          "
      timeout-minutes: 240
    
    - name: Test vLLM build
      run: |
        if ls output/vllm-*.whl 1> /dev/null 2>&1; then
          echo "✅ vLLM wheel构建成功"
          
          # 准备PyTorch安装命令
          if [ "${{ needs.prepare.outputs.build_pytorch }}" == "true" ] && [ -f pytorch-wheels/torch-*.whl ]; then
            PYTORCH_INSTALL="pip install /pytorch-wheels/torch-*.whl"
            PYTORCH_MOUNT="-v $(pwd)/pytorch-wheels:/pytorch-wheels"
          else
            PYTORCH_INSTALL="pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
            PYTORCH_MOUNT=""
          fi
          
          docker run --gpus all --rm \
            -v $(pwd)/output:/wheels \
            $PYTORCH_MOUNT \
            cu121-builder:latest bash -c "
              $PYTORCH_INSTALL
              pip install /wheels/vllm-*.whl
              python3 -c 'import vllm; print(f\"vLLM {vllm.__version__} installed successfully\")'
            "
        else
          echo "❌ vLLM wheel构建失败"
          exit 1
        fi
    
    - name: Upload vLLM artifacts
      uses: actions/upload-artifact@v4
      with:
        name: vllm-${{ needs.prepare.outputs.vllm_version }}-${{ matrix.gpu_type }}
        path: output/vllm-*.whl
        retention-days: 30

  create-combined-release:
    if: github.event_name == 'workflow_dispatch' || startsWith(github.ref, 'refs/tags/')
    runs-on: ubuntu-latest
    needs: [prepare, build-pytorch, build-vllm]
    # 允许部分job失败
    if: always() && (needs.build-pytorch.result == 'success' || needs.build-vllm.result == 'success')
    
    permissions:
      contents: write
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Download all artifacts
      uses: actions/download-artifact@v4
      with:
        path: artifacts/
    
    - name: Prepare release files
      run: |
        mkdir -p release/
        
        # 收集PyTorch wheels
        find artifacts/ -name "torch-*.whl" -exec cp {} release/ \; 2>/dev/null || true
        
        # 收集vLLM wheels  
        find artifacts/ -name "vllm-*.whl" -exec cp {} release/ \; 2>/dev/null || true
        
        # 创建安装脚本
        cat > release/install.sh << 'EOF'
        #!/bin/bash
        # 自动安装脚本
        
        set -e
        
        echo "=== PyTorch + vLLM CUDA 12.1 安装脚本 ==="
        
        # 检查GPU
        if ! command -v nvidia-smi &> /dev/null; then
            echo "警告: 未检测到nvidia-smi，请确保NVIDIA驱动已安装"
        else
            echo "检测到的GPU:"
            nvidia-smi --query-gpu=name --format=csv,noheader
        fi
        
        # 安装PyTorch
        if ls torch-*.whl 1> /dev/null 2>&1; then
            echo "安装PyTorch..."
            pip install torch-*.whl
        else
            echo "从PyTorch官方安装..."
            pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
        fi
        
        # 安装vLLM
        if ls vllm-*.whl 1> /dev/null 2>&1; then
            echo "安装vLLM..."
            pip install vllm-*.whl
        else
            echo "vLLM wheel未找到"
        fi
        
        # 验证安装
        echo "验证安装..."
        python3 -c "
        import torch
        print(f'PyTorch版本: {torch.__version__}')
        print(f'CUDA可用: {torch.cuda.is_available()}')
        if torch.cuda.is_available():
            print(f'CUDA版本: {torch.version.cuda}')
            print(f'GPU数量: {torch.cuda.device_count()}')
        
        try:
            import vllm
            print(f'vLLM版本: {vllm.__version__}')
        except ImportError:
            print('vLLM未安装')
        "
        
        echo "安装完成!"
        EOF
        
        chmod +x release/install.sh
        
        # 创建详细的release说明
        cat > release/RELEASE_NOTES.md << EOF
        # PyTorch ${{ needs.prepare.outputs.pytorch_version }} + vLLM ${{ needs.prepare.outputs.vllm_version }} for CUDA 12.1
        
        ## 构建信息
        - **PyTorch版本**: ${{ needs.prepare.outputs.pytorch_version }}
        - **vLLM版本**: ${{ needs.prepare.outputs.vllm_version }}
        - **CUDA版本**: 12.1.1
        - **Python版本**: 3.12
        - **支持GPU**: RTX 3090Ti, A100, A800, H20
        - **构建日期**: $(date -u +"%Y-%m-%d")
        
        ## 快速安装
        
        \`\`\`bash
        # 下载所有文件到当前目录
        # 运行自动安装脚本
        chmod +x install.sh
        ./install.sh
        \`\`\`
        
        ## 手动安装
        
        \`\`\`bash
        # 安装PyTorch
        pip install torch-*.whl
        
        # 安装vLLM
        pip install vllm-*.whl
        \`\`\`
        
        ## 验证安装
        
        \`\`\`python
        import torch
        import vllm
        
        print(f"PyTorch版本: {torch.__version__}")
        print(f"vLLM版本: {vllm.__version__}")
        print(f"CUDA可用: {torch.cuda.is_available()}")
        \`\`\`
        
        ## 使用示例
        
        \`\`\`python
        from vllm import LLM, SamplingParams
        
        # 创建vLLM实例
        llm = LLM(model="facebook/opt-125m")
        
        # 生成参数
        sampling_params = SamplingParams(temperature=0.8, top_p=0.95)
        
        # 生成文本
        prompts = ["Hello, my name is", "The future of AI is"]
        outputs = llm.generate(prompts, sampling_params)
        
        for output in outputs:
            prompt = output.prompt
            generated_text = output.outputs[0].text
            print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")
        \`\`\`
        
        ## 包含文件
        
        $(ls -la release/)
        
        ## 系统要求
        
        - NVIDIA GPU with Compute Capability 8.0+ (A100, A800, RTX 3090Ti, H20)
        - CUDA 12.1+
        - Python 3.12
        - 至少16GB GPU显存 (推荐24GB+)
        
        ## 故障排除
        
        如果遇到安装问题，请检查：
        1. NVIDIA驱动版本 >= 525.60.13
        2. CUDA版本兼容性
        3. Python版本为3.12
        4. 足够的GPU显存
        EOF
        
        echo "Release文件准备完成:"
        ls -la release/
    
    - name: Create Release
      uses: softprops/action-gh-release@v1
      with:
        tag_name: pytorch-${{ needs.prepare.outputs.pytorch_version }}-vllm-${{ needs.prepare.outputs.vllm_version }}-${{ github.run_number }}
        name: PyTorch ${{ needs.prepare.outputs.pytorch_version }} + vLLM ${{ needs.prepare.outputs.vllm_version }} for CUDA 12.1
        body_path: release/RELEASE_NOTES.md
        files: release/*
        draft: false
        prerelease: false
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
