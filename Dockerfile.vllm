# vLLM 0.10.0 编译镜像
FROM registry.cn-beijing.aliyuncs.com/yoce/cuda:cu121-build-prod AS vllm-builder

LABEL maintainer="yorelog <yorelog@gmail.com>"
LABEL description="vLLM 0.10.0 compiled with CUDA 12.1.1 for RTX 3090Ti, A100, A800, H20"

# 设置工作目录
WORKDIR /workspace

# 安装 vLLM 依赖
RUN uv pip install --python /opt/venv/bin/python \
    torch \
    transformers \
    tokenizers \
    fastapi \
    uvicorn \
    pydantic \
    ray \
    psutil \
    sentencepiece \
    numpy \
    requests \
    tqdm \
    py-cpuinfo \
    cupy-cuda12x

# 编译 vLLM 0.10.0
RUN echo "开始编译 vLLM 0.10.0..." && \
    cd /workspace && \
    git clone --branch v0.10.0 https://github.com/vllm-project/vllm.git vllm-0.10.0 && \
    cd vllm-0.10.0 && \
    \
    # 检测GPU并设置编译参数
    if nvidia-smi --query-gpu=name --format=csv,noheader,nounits | grep -q "3090"; then \
        export TORCH_CUDA_ARCH_LIST="8.6"; \
        export MAX_JOBS=8; \
    elif nvidia-smi --query-gpu=name --format=csv,noheader,nounits | grep -q "A100\|A800"; then \
        export TORCH_CUDA_ARCH_LIST="8.0"; \
        export MAX_JOBS=16; \
    elif nvidia-smi --query-gpu=name --format=csv,noheader,nounits | grep -q "H20"; then \
        export TORCH_CUDA_ARCH_LIST="8.9"; \
        export MAX_JOBS=20; \
    else \
        export TORCH_CUDA_ARCH_LIST="8.0;8.6;8.9"; \
        export MAX_JOBS=8; \
    fi && \
    \
    # 设置vLLM编译环境
    export VLLM_INSTALL_PUNICA_KERNELS=1 && \
    export VLLM_ENABLE_CUDA_GRAPHS=1 && \
    export CXXFLAGS="-O3" && \
    export CFLAGS="-O3" && \
    \
    # 编译
    python setup.py bdist_wheel && \
    \
    # 复制wheel文件到输出目录
    cp dist/*.whl /output/ && \
    \
    # 清理源码以减小镜像大小
    cd /workspace && \
    rm -rf vllm-0.10.0

# 验证编译结果
RUN echo "验证 vLLM 编译结果..." && \
    ls -la /output/ && \
    pip install /output/vllm-*.whl && \
    python -c "import vllm; print(f'vLLM version: {vllm.__version__}')"

# 设置默认命令
CMD ["ls", "-la", "/output"]
